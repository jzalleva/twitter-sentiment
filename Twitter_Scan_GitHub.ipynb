{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = \"MINE\"\n",
    "consumer_secret = \"MINE\"\n",
    "token = \"MINE\"\n",
    "\n",
    "access_token = 'MINE'\n",
    "access_token_secret = 'MINE'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, import necessary modules\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from datetime import datetime as dt, timedelta, timezone\n",
    "\n",
    "#Establish access to Twitter API\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are a number of functions below, which build on each other to eventually pull and rank tweets based on sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accesses Twitter based on user inputed search criteria - returns dictionary with 2 items \n",
    "# 1: List of 8 search results - one for each day. API search goes back a week, so it includes a partial 8th day\n",
    "# 2: Search term\n",
    "\n",
    "def get_tweets():\n",
    "    #Inputs for what you want to search and how many results to get\n",
    "    search_words = input(\"Input Search Here:\")\n",
    "    #Adjusts the format and number of results to work with the api\n",
    "    num_results = input(\"Sets of 100 Tweets Desired (Press enter for maximum):\")\n",
    "    print('--This may take a minute or two for a large number of Tweets --')\n",
    "    if num_results == '':\n",
    "        num_results = 180 #maximum pages for rate limit\n",
    "        print('Maximum - about 18,000 tweets')\n",
    "    elif int(num_results) < 7:\n",
    "        num_results = 7 #has to be at least 7 - one request per day\n",
    "    elif int(num_results) > 180:\n",
    "        num_results = 180 #maximum is 180 pages\n",
    "    else:\n",
    "        num_results = int(num_results)\n",
    "    num_results = int(num_results/7) #requests per date\n",
    "    #Gives list of dates in format for the API - results are up until but not including the date\n",
    "    date_list = []\n",
    "    current_date = dt.utcnow() \n",
    "    for i in range(7):\n",
    "        new_date = (current_date - timedelta(days=(i-1))).strftime(\"%Y-%m-%d\")\n",
    "        date_list.append(new_date)\n",
    "\n",
    "    #Creates a list of cursor items to later iterate on    \n",
    "    tweets = []\n",
    "    for date in date_list:\n",
    "        day_tweets = tw.Cursor(api.search, q = search_words, lang = \"en\", result_type = 'mixed', count = 100 ,until = date, tweet_mode = 'extended').pages(num_results)\n",
    "        tweets.append(day_tweets)\n",
    "    #Results are a dictionary with a list of tweets in cursor form and the user inputed search words    \n",
    "    return {'tweets':tweets, 'search_words':search_words}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieved tweets come with data in a dictionary. This function selects only the relevant keys and creates a new list\n",
    "def create_tweet_list(tweepy_cursor):\n",
    "    #Available dict keys for reference - #['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'metadata', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'retweeted_status', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'lang']\n",
    "    desired_keys = ['created_at', 'id', 'full_text', 'entities'\n",
    "                    , 'metadata', 'in_reply_to_status_id'\n",
    "                    ,'in_reply_to_user_id', 'user', 'place', 'retweeted_status'\n",
    "                    , 'retweet_count', 'favorite_count', 'retweeted']\n",
    "    #Need current date to filter old tweets - make variable now so it doesnt have to be created in each iteration below\n",
    "    current_date = dt.utcnow().replace(tzinfo=timezone.utc).replace(hour=0, minute = 0, second=0, microsecond=0)\n",
    "    #Create the list of tweets\n",
    "    tweet_list = []\n",
    "    for page in tweepy_cursor:\n",
    "        for status in page:\n",
    "            status = status._json #gets the dictionary object only\n",
    "            newdict = {} \n",
    "            #Filter out dates greater than a week old\n",
    "            if ( current_date - dt.strptime(status['created_at'], '%a %b %d %H:%M:%S %z %Y').replace(hour=0, minute = 0, second=0, microsecond=0) ).days > 6:\n",
    "                continue #this will loop back to the top if date is too old\n",
    "            else:                                                      \n",
    "                for key in desired_keys: #iterate through list of the keys we want\n",
    "                    try:\n",
    "                        newdict.update({ key : status[key] }) #adds new key:value pair to dictionary item for a status\n",
    "                    except KeyError: #loop back to key loop if there isn't a key present - used when there isnt a retweeted_status key\n",
    "                        continue\n",
    "                tweet_list.append(newdict)\n",
    "    return tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines previous two functions to get tweets and flatten the list of lists into something more manageable\n",
    "def compile_tweets():\n",
    "    pull_tweets = get_tweets() \n",
    "    search_results = pull_tweets['tweets'] #this returns the list of daily search results\n",
    "    result_list = []\n",
    "    #creates a list of lists with 7 elements\n",
    "    for daily_search_results in search_results:\n",
    "        new_results = create_tweet_list(daily_search_results) #this applies a function to refine that full list of tweets\n",
    "        result_list.append(new_results)\n",
    "    #flatten the list\n",
    "    big_list = []\n",
    "    for sublist in result_list:\n",
    "        for item in sublist:\n",
    "            big_list.append(item)\n",
    "    search_words = pull_tweets['search_words'] #returns the search term used to acquire the tweets\n",
    "    return {'tweets':big_list, 'search_words':search_words}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the list of tweets returned from the api search and adds them to the SQL \"Tweets2\" database\n",
    "\n",
    "def add_to_DB():\n",
    "    tweet_data = compile_tweets() #calls function to get list of tweets and the search term used\n",
    "    list_of_tweets = tweet_data['tweets']\n",
    "    search_words = tweet_data['search_words']\n",
    "    conn = sqlite3.connect('tweets2.sqlite')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    #Delete table if needed - only for when I'm testing stuff\n",
    "    cur.execute('''DROP TABLE IF EXISTS Tweets2''')\n",
    "\n",
    "    #Create the table and define data parameters\n",
    "    #Tweet ID is the primary key because we are interested in specific tweets\n",
    "    cur.execute('''CREATE TABLE IF NOT EXISTS Tweets2\n",
    "                    (tweet_id INTEGER PRIMARY KEY UNIQUE\n",
    "                    , user_id INTEGER\n",
    "                    , user_name TEXT, name TEXT\n",
    "                    , text TEXT , time_posted INTEGER\n",
    "                    , retweet_count INTEGER, favorite_count INTEGER\n",
    "                    , place_id TEXT, place_name TEXT, place_coord TEXT\n",
    "                    , search_words TEXT, sentiment REAL, subjectivity REAL, clean_text TEXT, clean_time INTEGER)''')\n",
    "\n",
    "\n",
    "    #Create values to input into SQL table\n",
    "    error_tweets = [] #list of dictionaries to hold all items where errors occur\n",
    "    count = 0\n",
    "    for tweet in list_of_tweets:\n",
    "        count += 1\n",
    "        #First - If status is a retweet, we want to ignore it\n",
    "        try:\n",
    "            tweet['retweeted_status']\n",
    "            continue #Want to restart loop if we run into a retweet\n",
    "        #If it is not a retweet, then will give KeyError, so use \n",
    "        except KeyError:\n",
    "            tweet_id = tweet['id']\n",
    "            user_id = tweet['user']['id']\n",
    "            user_name = tweet['user']['screen_name']\n",
    "            name = tweet['user']['name']\n",
    "            text = tweet['full_text']\n",
    "            time_posted = tweet['created_at']\n",
    "            retweets = tweet['retweet_count']\n",
    "            favorites = tweet['favorite_count']\n",
    "            try:\n",
    "                place_id = tweet['place']['id']\n",
    "            except:\n",
    "                place_id = None\n",
    "            try:\n",
    "                place_name = tweet['place']['full_name']\n",
    "            except:\n",
    "                place_name = None\n",
    "            try:\n",
    "                place_coord = tweet['place']['coordinates']\n",
    "            except:\n",
    "                place_coord = None\n",
    "        \n",
    "        #clean the text & re-format date to unix timestamp for sql\n",
    "        clean_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", text).split()) \n",
    "        clean_time = dt.strptime(time_posted, '%a %b %d %H:%M:%S %z %Y').timestamp()\n",
    "        \n",
    "        #check polarity\n",
    "        sentiment = TextBlob(clean_text).sentiment[0] # (-1) to 1 scale\n",
    "        subjectivity =  TextBlob(clean_text).sentiment[1] # 0-1 scale\n",
    "        \n",
    "        #Insert into SQL table - Tweets\n",
    "        try:\n",
    "            cur.execute('''INSERT OR IGNORE INTO Tweets2\n",
    "            (tweet_id, user_id, user_name, name, text, time_posted, retweet_count\n",
    "            , favorite_count, place_id, place_name, place_coord, search_words, sentiment, subjectivity, clean_text, clean_time) \n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)''',\n",
    "            (tweet_id, user_id, user_name, name, text, time_posted, retweets, favorites\n",
    "            , place_id, place_name, place_coord, search_words, sentiment, subjectivity, clean_text, clean_time) )\n",
    "\n",
    "        except: \n",
    "            print(\"error\", count)\n",
    "            error_dict = {\"tweet_id\":tweet_id, \"user_id\":user_id, \"user_name\":user_name, \"name\":name, \"text\":text, \"time_posted\":time_posted, \"retweet_count\":retweets, \"favorite_count\":favorites, \"place_id\":place_id, \"place_name\":place_name, \"place_coord\":place_coord}\n",
    "            error_tweets.append(error_dict)\n",
    "            pass\n",
    "\n",
    "    #Commit at the end of the list and notify with number of tweets added\n",
    "    conn.commit()\n",
    "    cur.execute('''SELECT COUNT(tweet_id) FROM Tweets2''')\n",
    "    num_tweets = cur.fetchone()[0] #if I dont do 0 it returns a one item list\n",
    "    print('Finished - ', num_tweets, \"tweets added to database out of\", len(list_of_tweets), \"tweets\" )\n",
    "          \n",
    "    cur.close() #always close the connection   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gives the sentiment for each day. Calculated as the average sentiment of all tweets and retweets\n",
    "def graph_data():\n",
    "    conn = sqlite3.connect('tweets2.sqlite')\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('''SELECT SUM(sentiment * (CASE WHEN retweet_count > 0 THEN retweet_count ELSE 1 END))\n",
    "                        /(SUM(retweet_count) + COUNT(tweet_id)) AS Sentiment\n",
    "                    , time_posted as Date\n",
    "                    , COUNT(tweet_id) as Num_Tweets\n",
    "                FROM Tweets2 \n",
    "                GROUP BY STRFTIME('%d', datetime(clean_time, 'unixepoch'))''') \n",
    "                #get the average sentiment of tweets that have a sentiment rating calculated\n",
    "\n",
    "    q = cur.fetchall()\n",
    "\n",
    "    #Create a list of results for the graph\n",
    "    daily_data = []\n",
    "    for item in q:\n",
    "        daily_list = []\n",
    "        date = item[1][0:10]\n",
    "        sentiment = round(item[0], 2)\n",
    "        daily_list = [date, sentiment]\n",
    "        daily_data.append(daily_list)\n",
    "    \n",
    "    cur.close()\n",
    "    return daily_data\n",
    "\n",
    "#Gives tweets positively affecting sentiment the most each day\n",
    "def pos_influencers():\n",
    "    conn = sqlite3.connect('tweets2.sqlite')\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    cur.execute('''SELECT MAX( sentiment * (1 + retweet_count) ), time_posted, retweet_count, sentiment, name, text\n",
    "                    FROM Tweets2\n",
    "                    GROUP BY strftime('%d', datetime(clean_time, 'unixepoch'))''')\n",
    "    \n",
    "    q2 = cur.fetchall()\n",
    "    \n",
    "    pos_influencers = []\n",
    "    for item in q2:\n",
    "        date = item[1][0:10]\n",
    "        retweets = item[2]\n",
    "        sentiment = round(item[3], 2)\n",
    "        name = item[4]\n",
    "        text = item[5]\n",
    "        new_list = [date, name, text, sentiment, retweets]\n",
    "        pos_influencers.append(new_list)  \n",
    "     \n",
    "\n",
    "\n",
    "    cur.close()\n",
    "    return pos_influencers\n",
    "\n",
    "#Gives tweets negatively affecting sentiment the most each day\n",
    "def neg_influencers():\n",
    "    conn = sqlite3.connect('tweets2.sqlite')\n",
    "    cur = conn.cursor()    \n",
    "    \n",
    "    cur.execute('''SELECT MIN( sentiment * (1 + retweet_count) ), time_posted, retweet_count, sentiment, name, text\n",
    "                FROM Tweets2\n",
    "                GROUP BY strftime('%d', datetime(clean_time, 'unixepoch'))''')    \n",
    "\n",
    "    q3 = cur.fetchall()\n",
    "    \n",
    "    neg_influencers = []\n",
    "    for item in q3:\n",
    "        date = item[1][0:10]\n",
    "        retweets = item[2]\n",
    "        sentiment = round(item[3], 2)\n",
    "        name = item[4]\n",
    "        text = item[5]\n",
    "        new_list = [date, name, text, sentiment, retweets]\n",
    "        neg_influencers.append(new_list) \n",
    "        \n",
    "    cur.close()\n",
    "    return neg_influencers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines positive nad negative influencer functions to print out the top tweets from each day\n",
    "def print_influencers():\n",
    "    pos = pos_influencers()\n",
    "    neg = neg_influencers()\n",
    "    print(\"Each Day's Most Influential Positive Tweets:\", '\\n', '----------------------')\n",
    "    count = 1\n",
    "    for i in pos:\n",
    "        if i[3] < 0:\n",
    "            print(count, '.', 'No positive sentiment Tweets retrieved', '\\n')\n",
    "            count += 1\n",
    "            continue\n",
    "        print(count, '.', i[0], '-', i[1], '-', i[2] )\n",
    "        print('Sentiment:', i[3], '-', 'Retweets:', i[4])\n",
    "        count += 1\n",
    "        print('\\n')\n",
    "    \n",
    "    print('----------------------')\n",
    "    \n",
    "    print(\"Each Day's Most Influential Negative Tweets:\", '\\n', '----------------------')\n",
    "    count = 1\n",
    "    for i in neg:\n",
    "        if i[3] > 0:\n",
    "            print(count, '.', 'No negative sentiment Tweets retrieved' ,'\\n')\n",
    "            count += 1\n",
    "            continue\n",
    "        print(count, '.', i[0], '-', i[1], '-', i[2] )\n",
    "        print('Sentiment:', i[3], '-', 'Retweets:', i[4])\n",
    "        count += 1\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Search Here: Tour de France\n",
      "Sets of 100 Tweets Desired (Press enter for maximum): \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--This may take a minute or two for a large number of Tweets --\n",
      "Maximum - about 18,000 tweets\n",
      "Finished -  2182 tweets added to database out of 11664 tweets\n",
      "\n",
      "\n",
      "Each Day's Most Influential Positive Tweets: \n",
      " ----------------------\n",
      "1 . Tue Sep 08 - RTÉ Sport - Breaking: Sam Bennett has become only the sixth Irishman to win a stage of the Tour de France following a sprint finish to a blisteringly fast 10th day of racing.\n",
      "\n",
      "https://t.co/DXVsrnlgNL\n",
      "Sentiment: 0.25 - Retweets: 248\n",
      "\n",
      "\n",
      "2 . Wed Sep 09 - Graham Watson - Visit the fantasy world of @EuroHoody at the Tour - it does make for some great reading. https://t.co/QCB0oLiXl7\n",
      "Sentiment: 0.8 - Retweets: 1\n",
      "\n",
      "\n",
      "3 . Thu Sep 10 - Cycling Weekly - 'I thought it was bike trouble, but it was just the legs': Julian Alaphilippe proves mortal once more on Tour de France stage 12 #TDF2020 | https://t.co/bj9oPlpNzp\n",
      "Sentiment: 0.2 - Retweets: 13\n",
      "\n",
      "\n",
      "4 . Fri Sep 11 - Kristen's watching le Tour de France 🚵🏻‍♂️ - @maladrift lol in the early days of the tour de france, cyclists would smoke during the races!! i cannot imagine!\n",
      "Sentiment: 0.45 - Retweets: 0\n",
      "\n",
      "\n",
      "5 . Sat Sep 12 - Thomas De Gendt - For sale. Pair of legs. Slightly used. Good enough to win any kind of race except Tour de France stages. Dm if interested.\n",
      "Sentiment: 0.36 - Retweets: 1982\n",
      "\n",
      "\n",
      "6 . Sun Sep 13 - Eurosport UK - \"Higuita is down - and we don't know why!\"\n",
      "\n",
      "🇫🇷 #TDF2020 LIVE\n",
      "📺 Eurosport 1\n",
      "📱 💻 Uninterrupted coverage: https://t.co/YhayffKVQN\n",
      "📃 Live blog updates: https://t.co/VLND45cTOe https://t.co/JYA46sLzOL\n",
      "Sentiment: 0.03 - Retweets: 57\n",
      "\n",
      "\n",
      "7 . Mon Sep 14 - VeloNews - Pogačar, the most aggressive rider of this year's Tour, is planning to attack race leader and countryman Primož Roglič all the way to Paris.\n",
      "\n",
      "#TDF2020 @LeTour \n",
      "\n",
      "https://t.co/SKQ9QepuSy\n",
      "Sentiment: 0.5 - Retweets: 5\n",
      "\n",
      "\n",
      "----------------------\n",
      "Each Day's Most Influential Negative Tweets: \n",
      " ----------------------\n",
      "1 . Tue Sep 08 - La Flamme Rouge - HLN reports at least one member of DQT is being re-tested. #TdF2020\n",
      "\n",
      "https://t.co/ZBYazHn3PP\n",
      "Sentiment: -0.3 - Retweets: 15\n",
      "\n",
      "\n",
      "2 . Wed Sep 09 - Bruce Davies - @SBS so no Tour de France highlights in the morning anymore? Just breathtakingly boring tennis on both channels 😫👎\n",
      "Sentiment: -1.0 - Retweets: 0\n",
      "\n",
      "\n",
      "3 . Thu Sep 10 - Ned Boulting - On the Tour de France route today, we will see once again from the air the ruined village of Oradour-sur-Glane, the site of the massacre of 642 villagers on 10 June 1944. It is a startling sight, and was recently defaced by vandals, who replaced the word \"martyrs\" with \"liars\". https://t.co/MaIkEbMm6O\n",
      "Sentiment: -0.25 - Retweets: 247\n",
      "\n",
      "\n",
      "4 . Fri Sep 11 - Eileen Dewey - @marklevinshow You know what else is an ninteresting coincidence?\n",
      "\n",
      "All three countries have teams in the Tour de France. I think they're basically the ONLY teams that are \"country\" teams, unless you count Astana. Weird.\n",
      "Sentiment: -0.25 - Retweets: 0\n",
      "\n",
      "\n",
      "5 . Sat Sep 12 - Cyclingnews.com - Bardet suffers 'small haemorrhage' after Tour de France crash\n",
      "\n",
      "https://t.co/2FR387MxZ1 #TDF2020 https://t.co/yTvJZUzWF8\n",
      "Sentiment: -0.42 - Retweets: 7\n",
      "\n",
      "\n",
      "6 . Sun Sep 13 - Reuters - Bernal at a loss after brutal Tour de France failure https://t.co/LSuGzENoQq https://t.co/rWUk5TNJV9\n",
      "Sentiment: -0.6 - Retweets: 6\n",
      "\n",
      "\n",
      "7 . Mon Sep 14 - Okedi Peter John - Late Ewan surge seals Tour de France stage three victory https://t.co/5aCU01gxUt\n",
      "Sentiment: -0.3 - Retweets: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Wraps everything up to print out the most influential tweets from each day.\n",
    "def get_top_influencers():\n",
    "    add_to_DB()\n",
    "    print('\\n')\n",
    "    print_influencers()\n",
    "\n",
    "get_top_influencers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
